{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Marketplace code \n",
    "\n",
    "########## Import required libraries ##########\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "########## SETUP SELENIUM ##########\n",
    "chrome_driver_path = \"/Users/fadil/Downloads/chromedriver-mac-arm642/chromedriver\"\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--user-data-dir=/Users/fadil/Library/Application Support/Google/Chrome\")\n",
    "chrome_options.add_argument(\"--profile-directory=Default\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# ‚úÖ Start Chrome Driver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "########## Function to Mimic Human Scrolling ##########\n",
    "def human_scroll(driver, times=5):\n",
    "    \"\"\"Scrolls randomly like a human user to load more posts.\"\"\"\n",
    "    for _ in range(times):\n",
    "        scroll_distance = random.randint(700, 900)  # Random scroll height\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_distance});\")\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "\n",
    "########## Navigate to Facebook Marketplace ##########\n",
    "marketplace_url = \"https://www.facebook.com/marketplace/search/?query=power%20bank\"\n",
    "driver.get(marketplace_url)\n",
    "time.sleep(5)\n",
    "human_scroll(driver, times=5)\n",
    "\n",
    "########## Extract Post Links from Marketplace ##########\n",
    "def extract_marketplace_posts():\n",
    "    \"\"\"Extracts individual post links from Facebook Marketplace.\"\"\"\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    posts = []\n",
    "\n",
    "    items = soup.find_all(\"a\", href=True)\n",
    "    \n",
    "    for item in items:\n",
    "        link = item[\"href\"]\n",
    "        if \"/marketplace/item/\" in link:  # Ensure it's a marketplace post\n",
    "            full_link = \"https://www.facebook.com\" + link.split(\"?\")[0]  # Remove tracking parameters\n",
    "            posts.append(full_link)\n",
    "\n",
    "    return list(set(posts))  # Remove duplicates\n",
    "\n",
    "# ‚úÖ Get all Marketplace post links\n",
    "marketplace_posts = extract_marketplace_posts()\n",
    "print(f\"üîç Found {len(marketplace_posts)} Marketplace Listings.\")\n",
    "\n",
    "########## Visit Each Marketplace Post and Extract Final Link ##########\n",
    "data = []\n",
    "for index, post_url in enumerate(marketplace_posts):\n",
    "    print(f\"üîó Visiting Post {index+1}/{len(marketplace_posts)}: {post_url}\")\n",
    "    \n",
    "    driver.get(post_url)\n",
    "    time.sleep(random.uniform(3, 6))  # Random delay for human-like interaction\n",
    "\n",
    "    # Store post details\n",
    "    data.append({\"Post Link\": post_url})\n",
    "\n",
    "########## Save results to CSV ##########\n",
    "df = pd.DataFrame(data)\n",
    "csv_filename = \"marketplace_powerbank_listings.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "########## Close browser ##########\n",
    "driver.quit()\n",
    "\n",
    "print(f\"\\n‚úÖ Facebook Marketplace Crawling Completed! Data saved to '{csv_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Pages code\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Setup Chrome WebDriver with profile\n",
    "CHROME_DRIVER_PATH = \"/Users/fadil/Downloads/chromedriver-mac-arm642/chromedriver\"\n",
    "PROGRESS_FILE = \"fb_crawler_progress.json\"\n",
    "OUTPUT_FILE = \"facebook_powerbank_posts_thai.csv\"\n",
    "\n",
    "\n",
    "# Keywords for search\n",
    "SEARCH_KEYWORDS = [\n",
    "    \"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\",\n",
    "    \"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Å‡πå\",\n",
    "    \"‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\",\n",
    "    \"‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Å‡πå\",\n",
    "    \"‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà‡∏™‡∏≥‡∏£‡∏≠‡∏á\",\n",
    "    \"Powerbank\",\n",
    "    \"‡πÅ‡∏ö‡∏ï‡∏™‡∏≥‡∏£‡∏≠‡∏á\",\n",
    "    \"Eloop\"\n",
    "]\n",
    "\n",
    "\n",
    "# Keywords for filtering posts\n",
    "FILTER_KEYWORDS = [\"Power bank\",\"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\",\"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Å‡πå\",\"‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\",\"‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Å‡πå\",\"‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà‡∏™‡∏≥‡∏£‡∏≠‡∏á\",\"Powerbank\",\"‡πÅ‡∏ö‡∏ï‡∏™‡∏≥‡∏£‡∏≠‡∏á\",\"Eloop\"]\n",
    "\n",
    "\n",
    "# Timing parameters\n",
    "PAGE_SEARCH_PAUSE_TIME = 5       # Time to wait between page search scrolls (seconds)\n",
    "PAGE_SEARCH_MAX_SCROLLS = 50     # Maximum number of scrolls when searching for pages\n",
    "\n",
    "POST_SCROLL_PAUSE_TIME = 5       # Time to wait between post scrolls (seconds) \n",
    "POST_SCROLL_MAX_ATTEMPTS = 50    # Maximum number of scrolls per page when looking for posts\n",
    "POST_NO_CHANGE_LIMIT = 10         # Number of consecutive scrolls with no new posts before stopping\n",
    "\n",
    "# Page load parameters\n",
    "PAGE_LOAD_WAIT_TIME = 5          # Time to wait after page loads (seconds)\n",
    "SAVE_PROGRESS_INTERVAL = 5       # Save progress after every X pages processed\n",
    "\n",
    "#################################################\n",
    "#           SCRIPT IMPLEMENTATION               #\n",
    "#################################################\n",
    "\n",
    "# Setup Chrome WebDriver with profile\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--user-data-dir=/Users/fadil/Library/Application Support/Google/Chrome\")\n",
    "chrome_options.add_argument(\"--profile-directory=Default\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # Disable Facebook notifications\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(CHROME_DRIVER_PATH)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Store extracted data\n",
    "all_post_urls = set()  # Avoid duplicate posts\n",
    "all_page_links = set()  # Avoid duplicate pages across ALL keywords\n",
    "completed_keywords = []  # Track which keywords we've already processed\n",
    "\n",
    "# Initialize or load progress tracking\n",
    "def init_progress():\n",
    "    global all_post_urls, all_page_links, completed_keywords\n",
    "    \n",
    "    if not os.path.exists(PROGRESS_FILE) or restart_mode:\n",
    "        progress = {\n",
    "            \"completed_keywords\": [],\n",
    "            \"page_links\": [],\n",
    "            \"post_urls\": []\n",
    "        }\n",
    "        # Create the file\n",
    "        with open(PROGRESS_FILE, 'w') as f:\n",
    "            json.dump(progress, f)\n",
    "        return progress\n",
    "    \n",
    "    try:\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "            # Load the saved data into our variables\n",
    "            all_post_urls = set(progress[\"post_urls\"])\n",
    "            all_page_links = set(progress[\"page_links\"])\n",
    "            completed_keywords = progress[\"completed_keywords\"]\n",
    "            return progress\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading progress file: {e}\")\n",
    "        print(\"Creating new progress file...\")\n",
    "        progress = {\n",
    "            \"completed_keywords\": [],\n",
    "            \"page_links\": [],\n",
    "            \"post_urls\": []\n",
    "        }\n",
    "        with open(PROGRESS_FILE, 'w') as f:\n",
    "            json.dump(progress, f)\n",
    "        return progress\n",
    "\n",
    "# Save progress\n",
    "def save_progress():\n",
    "    try:\n",
    "        progress = {\n",
    "            \"completed_keywords\": completed_keywords,\n",
    "            \"page_links\": list(all_page_links),\n",
    "            \"post_urls\": list(all_post_urls)\n",
    "        }\n",
    "        with open(PROGRESS_FILE, 'w') as f:\n",
    "            json.dump(progress, f)\n",
    "        \n",
    "        # Also save posts to CSV\n",
    "        df = pd.DataFrame(list(all_post_urls), columns=[\"Post URL\"])\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"üíæ Progress saved - {len(all_post_urls)} posts collected so far.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error saving progress: {e}\")\n",
    "\n",
    "# Function to extract pages and posts\n",
    "def extract_pages_and_posts(search_query):\n",
    "    \"\"\" Searches Facebook for pages, extracts relevant posts, and saves them \"\"\"\n",
    "    global all_post_urls, all_page_links, completed_keywords\n",
    "    \n",
    "    print(f\"\\nüîç Searching: {search_query}\")\n",
    "\n",
    "    # Open Facebook search for pages\n",
    "    search_url = f\"https://www.facebook.com/search/pages/?q={search_query.replace(' ', '%20')}\"\n",
    "    driver.get(search_url)\n",
    "    time.sleep(PAGE_LOAD_WAIT_TIME)  # Allow time for the page to load\n",
    "\n",
    "    # Scroll to load more pages (Auto-Stop if No New Content Appears)\n",
    "    previous_count = 0  # Track loaded pages count\n",
    "    stop_scroll_attempts = 0  # Count how many times no new pages load\n",
    "    \n",
    "    # New pages found in this specific search\n",
    "    new_page_links = set()\n",
    "\n",
    "    print(\"üìú Scrolling to find pages...\")\n",
    "    for scroll_num in range(1, PAGE_SEARCH_MAX_SCROLLS + 1):  # Configurable max scroll attempts\n",
    "        pages = driver.find_elements(By.XPATH, \"//a[contains(@href, 'facebook.com/') and not(contains(@href, 'profile.php'))]\")\n",
    "        current_count = len(pages)\n",
    "\n",
    "        # Show scrolling progress\n",
    "        print(f\"üìú Page search scroll #{scroll_num} - Found {current_count} potential pages\")\n",
    "\n",
    "        if current_count == previous_count:\n",
    "            stop_scroll_attempts += 1\n",
    "            print(f\"‚ö†Ô∏è No new pages found ({stop_scroll_attempts}/{POST_NO_CHANGE_LIMIT})\")\n",
    "        else:\n",
    "            stop_scroll_attempts = 0  # Reset if new pages appear\n",
    "            print(f\"‚úÖ Found {current_count - previous_count} new potential pages\")\n",
    "\n",
    "        if stop_scroll_attempts >= POST_NO_CHANGE_LIMIT:\n",
    "            print(\"‚úÖ No more new results. Stopping scroll.\")\n",
    "            break\n",
    "\n",
    "        previous_count = current_count\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(PAGE_SEARCH_PAUSE_TIME)  # Configurable pause time\n",
    "\n",
    "    # Extract only NEW page links that haven't been processed before\n",
    "    for page in pages:\n",
    "        try:\n",
    "            link = page.get_attribute(\"href\")\n",
    "            # Check if this is a valid FB link AND it hasn't been processed before\n",
    "            if link and \"facebook.com\" in link:\n",
    "                # Print if it's a duplicate or new page\n",
    "                if link in all_page_links:\n",
    "                    print(f\"üîÑ DUPLICATE PAGE: {link}\")\n",
    "                else:\n",
    "                    print(f\"üÜï NEW PAGE: {link}\")\n",
    "                    new_page_links.add(link)\n",
    "                    all_page_links.add(link)  # Add to global tracking set\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error extracting page link: {e}\")\n",
    "\n",
    "    print(f\"üîπ Found {len(new_page_links)} NEW Facebook pages for this keyword.\")\n",
    "    print(f\"üî∏ Skipped {len(pages) - len(new_page_links)} DUPLICATE pages.\")\n",
    "    print(f\"üîπ Total unique pages collected so far: {len(all_page_links)}\")\n",
    "\n",
    "    # Visit each NEW Facebook page found in this search\n",
    "    for index, page_link in enumerate(new_page_links):\n",
    "        try:\n",
    "            print(f\"üîπ Visiting NEW Page {index + 1}/{len(new_page_links)}: {page_link}\")\n",
    "\n",
    "            driver.get(page_link)  # Open the page link directly\n",
    "            time.sleep(PAGE_LOAD_WAIT_TIME)  # Configurable wait time\n",
    "\n",
    "            # Detailed scrolling with visual indicators\n",
    "            scroll_page_and_extract_posts(page_link)\n",
    "\n",
    "            # Save progress after configured interval\n",
    "            if (index + 1) % SAVE_PROGRESS_INTERVAL == 0:\n",
    "                save_progress()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error accessing page: {e}\")\n",
    "    \n",
    "    # Mark this keyword as completed\n",
    "    if search_query not in completed_keywords:\n",
    "        completed_keywords.append(search_query)\n",
    "    \n",
    "    # Save progress after each keyword\n",
    "    save_progress()\n",
    "\n",
    "def scroll_page_and_extract_posts(page_link):\n",
    "    \"\"\"Scroll a page with detailed progress and extract posts\"\"\"\n",
    "    global all_post_urls\n",
    "    \n",
    "    print(f\"üìú Starting to scroll page to load ALL posts...\")\n",
    "    previous_post_count = 0\n",
    "    consecutive_no_change = 0\n",
    "    posts = []\n",
    "    \n",
    "    # Add visual scroll progress indicator\n",
    "    print(\"üìä Scroll progress: [\", end=\"\")\n",
    "    \n",
    "    for scroll_attempt in range(1, POST_SCROLL_MAX_ATTEMPTS + 1):\n",
    "        # Scroll down\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(POST_SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        # Calculate current page height\n",
    "        current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Try to find posts with multiple different XPath patterns\n",
    "        posts = driver.find_elements(By.XPATH, \"//a[contains(@href, '/posts/') or contains(@href, '/photos/')]\")\n",
    "        \n",
    "        # If main pattern didn't work well, try alternative patterns\n",
    "        if len(posts) <= 1:\n",
    "            # Try alternative XPath to find more post elements\n",
    "            alt_posts = driver.find_elements(By.XPATH, \"//div[contains(@class, 'userContentWrapper')]//a[contains(@href, '/')]\")\n",
    "            if len(alt_posts) > len(posts):\n",
    "                posts = alt_posts\n",
    "                \n",
    "        current_post_count = len(posts)\n",
    "        \n",
    "        # Show scrolling progress\n",
    "        if scroll_attempt % 5 == 0:\n",
    "            print(\"=\", end=\"\", flush=True)\n",
    "            \n",
    "        # Detailed log\n",
    "        if scroll_attempt % 5 == 0 or current_post_count != previous_post_count:\n",
    "            print(f\"\\nüìú Scroll #{scroll_attempt} - Found {current_post_count} posts (Height: {current_height}px)\", end=\"\")\n",
    "            if current_post_count > previous_post_count:\n",
    "                print(f\" (+{current_post_count - previous_post_count} new posts)\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "        \n",
    "        # Check if we've reached the end (no new posts after multiple scrolls)\n",
    "        if current_post_count == previous_post_count:\n",
    "            consecutive_no_change += 1\n",
    "            if scroll_attempt % 5 == 0:\n",
    "                print(f\"‚ö†Ô∏è No new posts for {consecutive_no_change} consecutive scrolls\")\n",
    "            \n",
    "            if consecutive_no_change >= POST_NO_CHANGE_LIMIT:\n",
    "                print(f\"\\n‚úÖ No new posts after {POST_NO_CHANGE_LIMIT} consecutive scrolls. Reached end of page.\")\n",
    "                break\n",
    "        else:\n",
    "            consecutive_no_change = 0  # Reset counter if we found new posts\n",
    "            \n",
    "        previous_post_count = current_post_count\n",
    "    \n",
    "    print(\"] Complete\")\n",
    "    print(f\"‚úÖ Scrolling complete. Found {len(posts)} total posts on this page.\")\n",
    "\n",
    "    # Find and save post URLs\n",
    "    post_count = 0\n",
    "    for post in posts:\n",
    "        try:\n",
    "            post_url = post.get_attribute(\"href\")\n",
    "            post_text = post.text.lower() if post.text else \"\"\n",
    "\n",
    "            # Ensure URL is valid and contains at least one keyword\n",
    "            if post_url and any(keyword.lower() in post_text for keyword in FILTER_KEYWORDS):\n",
    "                if post_url not in all_post_urls:\n",
    "                    all_post_urls.add(post_url)\n",
    "                    post_count += 1\n",
    "                    print(f\"‚úÖ Found Post: {post_url}\")  # Print URL in real-time\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error extracting post: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Extracted {post_count} new posts from this page. Total posts so far: {len(all_post_urls)}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Check for restart flag\n",
    "    restart_mode = len(sys.argv) > 1 and sys.argv[1] == \"--restart\"\n",
    "    if restart_mode:\n",
    "        print(\"\\nüîÑ RESTART MODE: Starting fresh and clearing progress...\")\n",
    "        if os.path.exists(PROGRESS_FILE):\n",
    "            os.remove(PROGRESS_FILE)\n",
    "        if os.path.exists(OUTPUT_FILE):\n",
    "            backup_file = f\"{OUTPUT_FILE}.bak\"\n",
    "            if os.path.exists(backup_file):\n",
    "                os.remove(backup_file)\n",
    "            os.rename(OUTPUT_FILE, backup_file)\n",
    "            print(f\"Previous results backed up to {backup_file}\")\n",
    "    else:\n",
    "        print(\"\\n‚ñ∂Ô∏è RESUME MODE: Continuing from previous run...\")\n",
    "    \n",
    "    # Dictionary to store stats for each keyword\n",
    "    keyword_stats = {}\n",
    "    \n",
    "    try:\n",
    "        # Initialize/load progress\n",
    "        progress = init_progress()\n",
    "        print(f\"üìä Starting with {len(all_page_links)} known pages and {len(all_post_urls)} posts\")\n",
    "        print(f\"‚úÖ Already completed keywords: {completed_keywords}\")\n",
    "        \n",
    "        # Setup the initial output file if it doesn't exist\n",
    "        if not os.path.exists(OUTPUT_FILE):\n",
    "            with open(OUTPUT_FILE, 'w') as f:\n",
    "                f.write(\"Post URL\\n\")\n",
    "        \n",
    "        # Print configuration for user reference\n",
    "        print(\"\\n‚öôÔ∏è CURRENT CONFIGURATION:\")\n",
    "        print(f\"- Page search scroll pause time: {PAGE_SEARCH_PAUSE_TIME} seconds\")\n",
    "        print(f\"- Page search max scrolls: {PAGE_SEARCH_MAX_SCROLLS}\")\n",
    "        print(f\"- Post scroll pause time: {POST_SCROLL_PAUSE_TIME} seconds\")\n",
    "        print(f\"- Post scroll max attempts: {POST_SCROLL_MAX_ATTEMPTS}\")\n",
    "        print(f\"- Post no-change limit: {POST_NO_CHANGE_LIMIT}\")\n",
    "        print(f\"- Page load wait time: {PAGE_LOAD_WAIT_TIME} seconds\")\n",
    "        print(f\"- Save progress interval: Every {SAVE_PROGRESS_INTERVAL} pages\")\n",
    "        \n",
    "        # Process each keyword\n",
    "        for i, keyword in enumerate(SEARCH_KEYWORDS):\n",
    "            # Skip already completed keywords unless in restart mode\n",
    "            if keyword in completed_keywords and not restart_mode:\n",
    "                print(f\"\\n‚è≠Ô∏è Skipping already completed keyword: {keyword}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nüìå Processing keyword {i+1}/{len(SEARCH_KEYWORDS)}: {keyword}\")\n",
    "            \n",
    "            # Store page count before this keyword\n",
    "            pages_before = len(all_page_links)\n",
    "            posts_before = len(all_post_urls)\n",
    "            \n",
    "            # Process the keyword\n",
    "            extract_pages_and_posts(keyword)\n",
    "            \n",
    "            # Calculate statistics for this keyword\n",
    "            new_pages = len(all_page_links) - pages_before\n",
    "            new_posts = len(all_post_urls) - posts_before\n",
    "            keyword_stats[keyword] = {\n",
    "                \"new_pages\": new_pages,\n",
    "                \"total_pages_so_far\": len(all_page_links),\n",
    "                \"new_posts\": new_posts,\n",
    "                \"posts_so_far\": len(all_post_urls)\n",
    "            }\n",
    "            \n",
    "            # Save progress after each keyword (already done in extract_pages_and_posts)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during execution: {e}\")\n",
    "        # Save progress even if there's an error\n",
    "        save_progress()\n",
    "    finally:\n",
    "        # Print summary statistics\n",
    "        print(\"\\nüìà CRAWLING RESULTS üìà\")\n",
    "        print(\"-\" * 85)\n",
    "        print(f\"{'KEYWORD':<25} {'NEW PAGES':<12} {'NEW POSTS':<12} {'TOTAL PAGES':<12} {'TOTAL POSTS':<12}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        running_total_pages = 0\n",
    "        running_total_posts = 0\n",
    "        for keyword, stats in keyword_stats.items():\n",
    "            running_total_pages += stats[\"new_pages\"]\n",
    "            running_total_posts += stats[\"new_posts\"]\n",
    "            print(f\"{keyword:<25} {stats['new_pages']:<12} {stats['new_posts']:<12} {running_total_pages:<12} {running_total_posts:<12}\")\n",
    "        \n",
    "        print(\"-\" * 85)\n",
    "        print(f\"{'GRAND TOTAL':<25} {len(all_page_links):<12} {'':<12} {'':<12} {len(all_post_urls):<12}\")\n",
    "\n",
    "        print(\"\\nüéâ Crawling completed! Data saved to 'facebook_powerbank_posts_thai.csv'.\")\n",
    "        print(f\"üìä Final Statistics: Processed {len(all_page_links)} unique pages and found {len(all_post_urls)} relevant posts.\")\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##Pages and Groups\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Setup Chrome WebDriver with profile\n",
    "CHROME_DRIVER_PATH = \"/Users/fadil/Downloads/chromedriver-mac-arm642/chromedriver\"\n",
    "PROGRESS_FILE = \"fb_crawler_progress.json\"\n",
    "OUTPUT_FILE = \"facebook_powerbank_posts_groups_thai.csv\"\n",
    "\n",
    "\n",
    "# Keywords for search\n",
    "SEARCH_KEYWORDS = [\n",
    "    \"Power Bank\",\n",
    "    \"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\",\n",
    "    \"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Å‡πå\",\n",
    "    \"‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\",\n",
    "    \"‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Å‡πå\",\n",
    "    \"‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà‡∏™‡∏≥‡∏£‡∏≠‡∏á\",\n",
    "    \"Powerbank\",\n",
    "    \"‡πÅ‡∏ö‡∏ï‡∏™‡∏≥‡∏£‡∏≠‡∏á\",\n",
    "    \"Eloop\"\n",
    "]\n",
    "\n",
    "\n",
    "# Keywords for filtering posts\n",
    "FILTER_KEYWORDS = [\"Power Bank\",\"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\",\"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Å‡πå\",\"‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\",\"‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Å‡πå\",\"‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà‡∏™‡∏≥‡∏£‡∏≠‡∏á\",\"Powerbank\",\"‡πÅ‡∏ö‡∏ï‡∏™‡∏≥‡∏£‡∏≠‡∏á\",\"Eloop\", \"‡πÅ‡∏ö‡∏ï‡∏Ø\",  \"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå\",  \"‡∏û‡∏≤‡∏ß‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡πá‡∏á‡∏Ñ‡πå\", \"‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏á‡∏Ñ‡πå\", ]\n",
    "\n",
    "\n",
    "# Timing parameters\n",
    "PAGE_SEARCH_PAUSE_TIME = 10       # Time to wait between page search scrolls (seconds)\n",
    "PAGE_SEARCH_MAX_SCROLLS = 50     # Maximum number of scrolls when searching for pages\n",
    "GROUP_SEARCH_PAUSE_TIME = 10      # Time to wait between group search scrolls (seconds)\n",
    "GROUP_SEARCH_MAX_SCROLLS = 50    # Maximum number of scrolls when searching for groups\n",
    "\n",
    "POST_SCROLL_PAUSE_TIME = 10       # Time to wait between post scrolls (seconds) \n",
    "POST_SCROLL_MAX_ATTEMPTS = 50    # Maximum number of scrolls per page when looking for posts\n",
    "POST_NO_CHANGE_LIMIT = 10        # Number of consecutive scrolls with no new posts before stopping\n",
    "\n",
    "# Page load parameters\n",
    "PAGE_LOAD_WAIT_TIME = 10          # Time to wait after page loads (seconds)\n",
    "SAVE_PROGRESS_INTERVAL = 5       # Save progress after every X pages processed\n",
    "\n",
    "#################################################\n",
    "#           SCRIPT IMPLEMENTATION               #\n",
    "#################################################\n",
    "\n",
    "# Setup Chrome WebDriver with profile\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--user-data-dir=/Users/fadil/Library/Application Support/Google/Chrome\")\n",
    "chrome_options.add_argument(\"--profile-directory=Default\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # Disable Facebook notifications\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(CHROME_DRIVER_PATH)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Store extracted data\n",
    "all_post_urls = set()        # Avoid duplicate posts\n",
    "all_page_links = set()       # Avoid duplicate pages across ALL keywords\n",
    "all_group_links = set()      # Avoid duplicate groups across ALL keywords\n",
    "completed_keywords = []      # Track which keywords we've already processed\n",
    "\n",
    "# Initialize or load progress tracking\n",
    "def init_progress():\n",
    "    global all_post_urls, all_page_links, all_group_links, completed_keywords\n",
    "    \n",
    "    if not os.path.exists(PROGRESS_FILE) or restart_mode:\n",
    "        progress = {\n",
    "            \"completed_keywords\": [],\n",
    "            \"page_links\": [],\n",
    "            \"group_links\": [],\n",
    "            \"post_urls\": []\n",
    "        }\n",
    "        # Create the file\n",
    "        with open(PROGRESS_FILE, 'w') as f:\n",
    "            json.dump(progress, f)\n",
    "        return progress\n",
    "    \n",
    "    try:\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "            # Load the saved data into our variables\n",
    "            all_post_urls = set(progress[\"post_urls\"])\n",
    "            all_page_links = set(progress[\"page_links\"])\n",
    "            all_group_links = set(progress.get(\"group_links\", []))  # Handle older progress files\n",
    "            completed_keywords = progress[\"completed_keywords\"]\n",
    "            return progress\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading progress file: {e}\")\n",
    "        print(\"Creating new progress file...\")\n",
    "        progress = {\n",
    "            \"completed_keywords\": [],\n",
    "            \"page_links\": [],\n",
    "            \"group_links\": [],\n",
    "            \"post_urls\": []\n",
    "        }\n",
    "        with open(PROGRESS_FILE, 'w') as f:\n",
    "            json.dump(progress, f)\n",
    "        return progress\n",
    "\n",
    "# Save progress\n",
    "def save_progress():\n",
    "    try:\n",
    "        progress = {\n",
    "            \"completed_keywords\": completed_keywords,\n",
    "            \"page_links\": list(all_page_links),\n",
    "            \"group_links\": list(all_group_links),\n",
    "            \"post_urls\": list(all_post_urls)\n",
    "        }\n",
    "        with open(PROGRESS_FILE, 'w') as f:\n",
    "            json.dump(progress, f)\n",
    "        \n",
    "        # Also save posts to CSV\n",
    "        df = pd.DataFrame(list(all_post_urls), columns=[\"Post URL\"])\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"üíæ Progress saved - {len(all_post_urls)} posts collected so far.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error saving progress: {e}\")\n",
    "\n",
    "# Function to search for FB pages\n",
    "def find_pages_for_keyword(search_query):\n",
    "    \"\"\" Searches Facebook for pages with the given keyword \"\"\"\n",
    "    global all_page_links\n",
    "    \n",
    "    print(f\"\\nüîç Searching for PAGES with keyword: {search_query}\")\n",
    "\n",
    "    # Open Facebook search for pages\n",
    "    search_url = f\"https://www.facebook.com/search/pages/?q={search_query.replace(' ', '%20')}\"\n",
    "    driver.get(search_url)\n",
    "    time.sleep(PAGE_LOAD_WAIT_TIME)  # Allow time for the page to load\n",
    "\n",
    "    # Scroll to load more pages (Auto-Stop if No New Content Appears)\n",
    "    previous_count = 0  # Track loaded pages count\n",
    "    stop_scroll_attempts = 0  # Count how many times no new pages load\n",
    "    \n",
    "    # New pages found in this specific search\n",
    "    new_page_links = set()\n",
    "\n",
    "    print(\"üìú Scrolling to find pages...\")\n",
    "    for scroll_num in range(1, PAGE_SEARCH_MAX_SCROLLS + 1):  # Configurable max scroll attempts\n",
    "        pages = driver.find_elements(By.XPATH, \"//a[contains(@href, 'facebook.com/') and not(contains(@href, 'profile.php'))]\")\n",
    "        current_count = len(pages)\n",
    "\n",
    "        # Show scrolling progress\n",
    "        print(f\"üìú Page search scroll #{scroll_num} - Found {current_count} potential pages\")\n",
    "\n",
    "        if current_count == previous_count:\n",
    "            stop_scroll_attempts += 1\n",
    "            print(f\"‚ö†Ô∏è No new pages found ({stop_scroll_attempts}/{POST_NO_CHANGE_LIMIT})\")\n",
    "        else:\n",
    "            stop_scroll_attempts = 0  # Reset if new pages appear\n",
    "            print(f\"‚úÖ Found {current_count - previous_count} new potential pages\")\n",
    "\n",
    "        if stop_scroll_attempts >= POST_NO_CHANGE_LIMIT:\n",
    "            print(\"‚úÖ No more new results. Stopping scroll.\")\n",
    "            break\n",
    "\n",
    "        previous_count = current_count\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(PAGE_SEARCH_PAUSE_TIME)  # Configurable pause time\n",
    "\n",
    "    # Extract only NEW page links that haven't been processed before\n",
    "    for page in pages:\n",
    "        try:\n",
    "            link = page.get_attribute(\"href\")\n",
    "            # Check if this is a valid FB link AND it hasn't been processed before\n",
    "            if link and \"facebook.com\" in link:\n",
    "                # Filter out non-page links\n",
    "                if \"facebook.com/groups\" in link:\n",
    "                    continue  # Skip groups here, we'll handle them separately\n",
    "                \n",
    "                # Print if it's a duplicate or new page\n",
    "                if link in all_page_links:\n",
    "                    print(f\"üîÑ DUPLICATE PAGE: {link}\")\n",
    "                else:\n",
    "                    print(f\"üÜï NEW PAGE: {link}\")\n",
    "                    new_page_links.add(link)\n",
    "                    all_page_links.add(link)  # Add to global tracking set\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error extracting page link: {e}\")\n",
    "\n",
    "    print(f\"üîπ Found {len(new_page_links)} NEW Facebook pages for this keyword.\")\n",
    "    print(f\"üî∏ Skipped {len(pages) - len(new_page_links)} DUPLICATE pages.\")\n",
    "    print(f\"üîπ Total unique pages collected so far: {len(all_page_links)}\")\n",
    "    \n",
    "    return new_page_links\n",
    "\n",
    "# Function to search for FB groups\n",
    "def find_groups_for_keyword(search_query):\n",
    "    \"\"\" Searches Facebook for groups with the given keyword \"\"\"\n",
    "    global all_group_links\n",
    "    \n",
    "    print(f\"\\nüîç Searching for GROUPS with keyword: {search_query}\")\n",
    "\n",
    "    # Open Facebook search for groups\n",
    "    search_url = f\"https://www.facebook.com/search/groups/?q={search_query.replace(' ', '%20')}\"\n",
    "    driver.get(search_url)\n",
    "    time.sleep(PAGE_LOAD_WAIT_TIME)  # Allow time for the page to load\n",
    "\n",
    "    # Scroll to load more groups\n",
    "    previous_count = 0\n",
    "    stop_scroll_attempts = 0\n",
    "    new_group_links = set()\n",
    "\n",
    "    print(\"üìú Scrolling to find groups...\")\n",
    "    for scroll_num in range(1, GROUP_SEARCH_MAX_SCROLLS + 1):\n",
    "        # Find all group links\n",
    "        groups = driver.find_elements(By.XPATH, \"//a[contains(@href, 'facebook.com/groups/')]\")\n",
    "        current_count = len(groups)\n",
    "\n",
    "        # Show scrolling progress\n",
    "        print(f\"üìú Group search scroll #{scroll_num} - Found {current_count} potential groups\")\n",
    "\n",
    "        if current_count == previous_count:\n",
    "            stop_scroll_attempts += 1\n",
    "            print(f\"‚ö†Ô∏è No new groups found ({stop_scroll_attempts}/{POST_NO_CHANGE_LIMIT})\")\n",
    "        else:\n",
    "            stop_scroll_attempts = 0\n",
    "            print(f\"‚úÖ Found {current_count - previous_count} new potential groups\")\n",
    "\n",
    "        if stop_scroll_attempts >= POST_NO_CHANGE_LIMIT:\n",
    "            print(\"‚úÖ No more new group results. Stopping scroll.\")\n",
    "            break\n",
    "\n",
    "        previous_count = current_count\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(GROUP_SEARCH_PAUSE_TIME)\n",
    "\n",
    "    # Extract only NEW group links\n",
    "    for group in groups:\n",
    "        try:\n",
    "            link = group.get_attribute(\"href\")\n",
    "            # Ensure it's a valid group link\n",
    "            if link and \"facebook.com/groups/\" in link:\n",
    "                # Check if already processed\n",
    "                if link in all_group_links:\n",
    "                    print(f\"üîÑ DUPLICATE GROUP: {link}\")\n",
    "                else:\n",
    "                    print(f\"üÜï NEW GROUP: {link}\")\n",
    "                    new_group_links.add(link)\n",
    "                    all_group_links.add(link)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error extracting group link: {e}\")\n",
    "\n",
    "    print(f\"üîπ Found {len(new_group_links)} NEW Facebook groups for this keyword.\")\n",
    "    print(f\"üî∏ Skipped {len(groups) - len(new_group_links)} DUPLICATE groups.\")\n",
    "    print(f\"üîπ Total unique groups collected so far: {len(all_group_links)}\")\n",
    "    \n",
    "    return new_group_links\n",
    "\n",
    "# Function to check if a group is public\n",
    "def is_group_public(driver, group_link):\n",
    "    \"\"\"Determine if a Facebook group is public or private\"\"\"\n",
    "    try:\n",
    "        driver.get(group_link)\n",
    "        time.sleep(PAGE_LOAD_WAIT_TIME)\n",
    "        \n",
    "        # Look for join button - if present, it's likely private\n",
    "        join_buttons = driver.find_elements(By.XPATH, \"//div[contains(text(), 'Join group')]\")\n",
    "        if join_buttons:\n",
    "            # Check if there's any indicator this is a private group\n",
    "            private_indicators = driver.find_elements(By.XPATH, \"//span[contains(text(), 'Private group')]\")\n",
    "            if private_indicators:\n",
    "                print(f\"üîí Private group detected: {group_link}\")\n",
    "                return False\n",
    "                \n",
    "        # Check if we can see posts - a sign it's public or we're already a member\n",
    "        posts = driver.find_elements(By.XPATH, \"//a[contains(@href, '/posts/') or contains(@href, '/photos/')]\")\n",
    "        if posts:\n",
    "            print(f\"üåê Public group or member of group: {group_link}\")\n",
    "            return True\n",
    "            \n",
    "        # If in doubt, try scrolling to see if content loads\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(3)\n",
    "        posts = driver.find_elements(By.XPATH, \"//a[contains(@href, '/posts/') or contains(@href, '/photos/')]\")\n",
    "        if posts:\n",
    "            print(f\"üåê Confirmed public or member group: {group_link}\")\n",
    "            return True\n",
    "            \n",
    "        print(f\"üîí Likely private group (cannot view content): {group_link}\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error checking group type: {e}\")\n",
    "        return False  # Assume private/inaccessible if there's an error\n",
    "\n",
    "def scroll_page_and_extract_posts(source_link, source_type=\"page\"):\n",
    "    \"\"\"Scroll a page or group with detailed progress and extract posts\"\"\"\n",
    "    global all_post_urls\n",
    "    \n",
    "    print(f\"üìú Starting to scroll {source_type} to load ALL posts...\")\n",
    "    previous_post_count = 0\n",
    "    consecutive_no_change = 0\n",
    "    posts = []\n",
    "    \n",
    "    # Add visual scroll progress indicator\n",
    "    print(\"üìä Scroll progress: [\", end=\"\")\n",
    "    \n",
    "    for scroll_attempt in range(1, POST_SCROLL_MAX_ATTEMPTS + 1):\n",
    "        # Scroll down\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
    "        time.sleep(POST_SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        # Calculate current page height\n",
    "        current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Try to find posts with multiple different XPath patterns\n",
    "        posts = driver.find_elements(By.XPATH, \"//a[contains(@href, '/posts/') or contains(@href, '/photos/')]\")\n",
    "        \n",
    "        # If main pattern didn't work well, try alternative patterns\n",
    "        if len(posts) <= 1:\n",
    "            # Try alternative XPath to find more post elements\n",
    "            alt_posts = driver.find_elements(By.XPATH, \"//div[contains(@class, 'userContentWrapper')]//a[contains(@href, '/')]\")\n",
    "            if len(alt_posts) > len(posts):\n",
    "                posts = alt_posts\n",
    "                \n",
    "        current_post_count = len(posts)\n",
    "        \n",
    "        # Show scrolling progress\n",
    "        if scroll_attempt % 5 == 0:\n",
    "            print(\"=\", end=\"\", flush=True)\n",
    "            \n",
    "        # Detailed log\n",
    "        if scroll_attempt % 5 == 0 or current_post_count != previous_post_count:\n",
    "            print(f\"\\nüìú Scroll #{scroll_attempt} - Found {current_post_count} posts (Height: {current_height}px)\", end=\"\")\n",
    "            if current_post_count > previous_post_count:\n",
    "                print(f\" (+{current_post_count - previous_post_count} new posts)\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "        \n",
    "        # Check if we've reached the end (no new posts after multiple scrolls)\n",
    "        if current_post_count == previous_post_count:\n",
    "            consecutive_no_change += 1\n",
    "            if scroll_attempt % 5 == 0:\n",
    "                print(f\"‚ö†Ô∏è No new posts for {consecutive_no_change} consecutive scrolls\")\n",
    "            \n",
    "            if consecutive_no_change >= POST_NO_CHANGE_LIMIT:\n",
    "                print(f\"\\n‚úÖ No new posts after {POST_NO_CHANGE_LIMIT} consecutive scrolls. Reached end of {source_type}.\")\n",
    "                break\n",
    "        else:\n",
    "            consecutive_no_change = 0  # Reset counter if we found new posts\n",
    "            \n",
    "        previous_post_count = current_post_count\n",
    "    \n",
    "    print(\"] Complete\")\n",
    "    print(f\"‚úÖ Scrolling complete. Found {len(posts)} total posts on this {source_type}.\")\n",
    "\n",
    "    # Find and save post URLs\n",
    "    post_count = 0\n",
    "    # Replace the current post filtering method with this more robust version\n",
    "    for post in posts:\n",
    "        try:\n",
    "            post_url = post.get_attribute(\"href\")\n",
    "            \n",
    "            # Get text from the post element and its children\n",
    "            try:\n",
    "                # Try to get full element HTML to search within\n",
    "                post_html = post.get_attribute(\"outerHTML\")\n",
    "                # Also try direct text in case HTML isn't accessible\n",
    "                post_text = post.text.lower() if post.text else \"\"\n",
    "                \n",
    "                # Find parent container to get more context\n",
    "                parent = driver.execute_script(\"return arguments[0].parentNode;\", post)\n",
    "                parent_text = \"\"\n",
    "                if parent:\n",
    "                    parent_text = parent.text.lower() if parent.text else \"\"\n",
    "                    \n",
    "                # Check both post text and HTML content for keywords\n",
    "                text_to_search = (post_text + \" \" + parent_text).lower()\n",
    "                html_to_search = post_html.lower() if post_html else \"\"\n",
    "                \n",
    "                # If any keyword is found in either text or HTML, consider it a match\n",
    "                if post_url and (\n",
    "                    any(keyword.lower() in text_to_search for keyword in FILTER_KEYWORDS) or\n",
    "                    any(keyword.lower() in html_to_search for keyword in FILTER_KEYWORDS)\n",
    "                ):\n",
    "                    if post_url not in all_post_urls:\n",
    "                        all_post_urls.add(post_url)\n",
    "                        post_count += 1\n",
    "                        print(f\"‚úÖ Found Post: {post_url}\")\n",
    "                        \n",
    "                        # Debug info to see what was matched\n",
    "                        matched_keywords = [k for k in FILTER_KEYWORDS if k.lower() in text_to_search or k.lower() in html_to_search]\n",
    "                        print(f\"   ‚Ü™ Matched keywords: {', '.join(matched_keywords)}\")\n",
    "            except:\n",
    "                # Fallback method if advanced extraction fails\n",
    "                # Just check if the URL itself contains any keywords as a last resort\n",
    "                if post_url and any(keyword.lower() in post_url.lower() for keyword in FILTER_KEYWORDS):\n",
    "                    if post_url not in all_post_urls:\n",
    "                        all_post_urls.add(post_url)\n",
    "                        post_count += 1\n",
    "                        print(f\"‚úÖ Found Post (URL match): {post_url}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error extracting post: {e}\")\n",
    "# Process a keyword\n",
    "def process_keyword(keyword):\n",
    "    global all_post_urls, all_page_links, all_group_links, completed_keywords\n",
    "    \n",
    "    print(f\"\\nüìå Processing keyword: {keyword}\")\n",
    "    \n",
    "    # Store counts before this keyword\n",
    "    pages_before = len(all_page_links)\n",
    "    groups_before = len(all_group_links)\n",
    "    posts_before = len(all_post_urls)\n",
    "    \n",
    "    # Step 1: Find pages for this keyword\n",
    "    new_pages = find_pages_for_keyword(keyword)\n",
    "    \n",
    "    # Step 2: Find groups for this keyword\n",
    "    new_groups = find_groups_for_keyword(keyword)\n",
    "    \n",
    "    # Step 3: Process each page\n",
    "    print(f\"\\nüîç Processing {len(new_pages)} pages for keyword: {keyword}\")\n",
    "    pages_processed = 0\n",
    "    \n",
    "    for index, page_link in enumerate(new_pages):\n",
    "        try:\n",
    "            print(f\"üîπ Visiting PAGE {index + 1}/{len(new_pages)}: {page_link}\")\n",
    "            driver.get(page_link)\n",
    "            time.sleep(PAGE_LOAD_WAIT_TIME)\n",
    "            \n",
    "            # Extract posts from this page\n",
    "            scroll_page_and_extract_posts(page_link, \"page\")\n",
    "            pages_processed += 1\n",
    "            \n",
    "            # Save progress periodically\n",
    "            if pages_processed % SAVE_PROGRESS_INTERVAL == 0:\n",
    "                save_progress()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing page {page_link}: {e}\")\n",
    "    \n",
    "    # Step 4: Process each public group\n",
    "    print(f\"\\nüîç Processing {len(new_groups)} groups for keyword: {keyword}\")\n",
    "    groups_processed = 0\n",
    "    public_groups = 0\n",
    "    \n",
    "    for index, group_link in enumerate(new_groups):\n",
    "        try:\n",
    "            print(f\"üîπ Checking GROUP {index + 1}/{len(new_groups)}: {group_link}\")\n",
    "            \n",
    "            # Check if it's a public group\n",
    "            if is_group_public(driver, group_link):\n",
    "                print(f\"üåê Processing PUBLIC group: {group_link}\")\n",
    "                scroll_page_and_extract_posts(group_link, \"group\")\n",
    "                public_groups += 1\n",
    "            else:\n",
    "                print(f\"üîí Skipping PRIVATE group: {group_link}\")\n",
    "                \n",
    "            groups_processed += 1\n",
    "            \n",
    "            # Save progress periodically\n",
    "            if groups_processed % SAVE_PROGRESS_INTERVAL == 0:\n",
    "                save_progress()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing group {group_link}: {e}\")\n",
    "    \n",
    "    # Calculate statistics for this keyword\n",
    "    new_page_count = len(all_page_links) - pages_before\n",
    "    new_group_count = len(all_group_links) - groups_before\n",
    "    new_post_count = len(all_post_urls) - posts_before\n",
    "    \n",
    "    print(f\"\\n‚úÖ Keyword '{keyword}' complete:\")\n",
    "    print(f\"  - Pages: {new_page_count} new (processed {pages_processed})\")\n",
    "    print(f\"  - Groups: {new_group_count} new (processed {groups_processed}, {public_groups} public)\")\n",
    "    print(f\"  - Posts: {new_post_count} new\")\n",
    "    \n",
    "    # Mark this keyword as completed\n",
    "    if keyword not in completed_keywords:\n",
    "        completed_keywords.append(keyword)\n",
    "    \n",
    "    # Save progress after completing the keyword\n",
    "    save_progress()\n",
    "    \n",
    "    return {\n",
    "        \"new_pages\": new_page_count,\n",
    "        \"total_pages\": len(all_page_links),\n",
    "        \"new_groups\": new_group_count,\n",
    "        \"total_groups\": len(all_group_links),\n",
    "        \"public_groups\": public_groups,\n",
    "        \"new_posts\": new_post_count,\n",
    "        \"total_posts\": len(all_post_urls)\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Check for restart flag\n",
    "    restart_mode = len(sys.argv) > 1 and sys.argv[1] == \"--restart\"\n",
    "    if restart_mode:\n",
    "        print(\"\\nüîÑ RESTART MODE: Starting fresh and clearing progress...\")\n",
    "        if os.path.exists(PROGRESS_FILE):\n",
    "            os.remove(PROGRESS_FILE)\n",
    "        if os.path.exists(OUTPUT_FILE):\n",
    "            backup_file = f\"{OUTPUT_FILE}.bak\"\n",
    "            if os.path.exists(backup_file):\n",
    "                os.remove(backup_file)\n",
    "            os.rename(OUTPUT_FILE, backup_file)\n",
    "            print(f\"Previous results backed up to {backup_file}\")\n",
    "    else:\n",
    "        print(\"\\n‚ñ∂Ô∏è RESUME MODE: Continuing from previous run...\")\n",
    "    \n",
    "    # Dictionary to store stats for each keyword\n",
    "    keyword_stats = {}\n",
    "    \n",
    "    try:\n",
    "        # Initialize/load progress\n",
    "        progress = init_progress()\n",
    "        print(f\"üìä Starting with {len(all_page_links)} pages, {len(all_group_links)} groups, {len(all_post_urls)} posts\")\n",
    "        print(f\"‚úÖ Already completed keywords: {completed_keywords}\")\n",
    "        \n",
    "        # Setup the initial output file if it doesn't exist\n",
    "        if not os.path.exists(OUTPUT_FILE):\n",
    "            with open(OUTPUT_FILE, 'w') as f:\n",
    "                f.write(\"Post URL\\n\")\n",
    "        \n",
    "        # Print configuration for user reference\n",
    "        print(\"\\n‚öôÔ∏è CURRENT CONFIGURATION:\")\n",
    "        print(f\"- Page search pause time: {PAGE_SEARCH_PAUSE_TIME} seconds\")\n",
    "        print(f\"- Page search max scrolls: {PAGE_SEARCH_MAX_SCROLLS}\")\n",
    "        print(f\"- Group search pause time: {GROUP_SEARCH_PAUSE_TIME} seconds\")\n",
    "        print(f\"- Group search max scrolls: {GROUP_SEARCH_MAX_SCROLLS}\")\n",
    "        print(f\"- Post scroll pause time: {POST_SCROLL_PAUSE_TIME} seconds\")\n",
    "        print(f\"- Post scroll max attempts: {POST_SCROLL_MAX_ATTEMPTS}\")\n",
    "        print(f\"- Post no-change limit: {POST_NO_CHANGE_LIMIT}\")\n",
    "        print(f\"- Page load wait time: {PAGE_LOAD_WAIT_TIME} seconds\")\n",
    "        print(f\"- Save progress interval: Every {SAVE_PROGRESS_INTERVAL} items\")\n",
    "        \n",
    "        # Process each keyword\n",
    "        for i, keyword in enumerate(SEARCH_KEYWORDS):\n",
    "            # Skip already completed keywords unless in restart mode\n",
    "            if keyword in completed_keywords and not restart_mode:\n",
    "                print(f\"\\n‚è≠Ô∏è Skipping already completed keyword: {keyword}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nüìå Processing keyword {i+1}/{len(SEARCH_KEYWORDS)}: {keyword}\")\n",
    "            \n",
    "            # Process the keyword (find and crawl pages and groups)\n",
    "            stats = process_keyword(keyword)\n",
    "            keyword_stats[keyword] = stats\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during execution: {e}\")\n",
    "        # Save progress even if there's an error\n",
    "        save_progress()\n",
    "    finally:\n",
    "        # Print summary statistics\n",
    "        print(\"\\nüìà CRAWLING RESULTS üìà\")\n",
    "        print(\"-\" * 120)\n",
    "        print(f\"{'KEYWORD':<20} {'NEW PAGES':<10} {'NEW GROUPS':<10} {'PUBLIC GRPS':<10} {'NEW POSTS':<10} {'TOTAL PAGES':<10} {'TOTAL GROUPS':<10} {'TOTAL POSTS':<10}\")\n",
    "        print(\"-\" * 120)\n",
    "        \n",
    "        running_total_pages = 0\n",
    "        running_total_groups = 0\n",
    "        running_total_posts = 0\n",
    "        \n",
    "        for keyword, stats in keyword_stats.items():\n",
    "            running_total_pages += stats[\"new_pages\"]\n",
    "            running_total_groups += stats[\"new_groups\"]\n",
    "            running_total_posts += stats[\"new_posts\"]\n",
    "            \n",
    "            print(f\"{keyword:<20} {stats['new_pages']:<10} {stats['new_groups']:<10} {stats.get('public_groups', 0):<10} \" +\n",
    "                  f\"{stats['new_posts']:<10} {running_total_pages:<10} {running_total_groups:<10} {running_total_posts:<10}\")\n",
    "        \n",
    "        print(\"-\" * 120)\n",
    "        print(f\"{'GRAND TOTAL':<20} {len(all_page_links):<10} {len(all_group_links):<10} {'':<10} {len(all_post_urls):<10}\")\n",
    "\n",
    "        print(\"\\nüéâ Crawling completed! Data saved to 'facebook_powerbank_posts_thai.csv'.\")\n",
    "        print(f\"üìä Final Statistics: Processed {len(all_page_links)} pages, {len(all_group_links)} groups, and found {len(all_post_urls)} relevant posts.\")\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
